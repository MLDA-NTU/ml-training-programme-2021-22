{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autograd_with_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP6bAO4CiAL5"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbCncQJFeJvv"
      },
      "source": [
        "Deep learning deals with **tensors**. They are just a fancier way to call multi-dimensional array. Matrices and vectors are also tensors (rank-2 tensor and rank-1 tensor respectively)\n",
        "\n",
        "To use **autograd** (automatic gradient) feature in PyTorch, we must perform operations on `torch.Tensor`\n",
        "\n",
        "$$x = 5$$\n",
        "\n",
        "$$y = x^2$$\n",
        "\n",
        "$$\\frac{dy}{dx} = 2x = 10$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKo_AbX0iNnU",
        "outputId": "028424a6-0be0-4d08-f6a5-cdf9c0b7f6a9"
      },
      "source": [
        "x = torch.tensor(5., requires_grad=True)        # create a tensor. set requires_grad=True to enable differentiation wrt to x\n",
        "x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGWqQvXxiU2G",
        "outputId": "28877562-285b-46ec-8b68-b94fc0e650c6"
      },
      "source": [
        "y = x**2            # calculate y from x\n",
        "y"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(25., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW-ivHABiYuN"
      },
      "source": [
        "y.backward()        # this will calculate dy/dx"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjxoIiPLimMY",
        "outputId": "ea661139-8378-4a8b-e4cc-6e78a21bcc61"
      },
      "source": [
        "x.grad              # the result is stored in x.grad i.e. 2 x 5 = 10"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEmkLKrIfB3K"
      },
      "source": [
        "This becomes more powerful when we deal with a lot of numbers e.g. vectors and matrices\n",
        "\n",
        "$$\\vec{x} = [1 \\space 2 \\space 3]$$\n",
        "\n",
        "$$y = \\sum_{i=1}^{3}x_i^2 = (x_1^2 + x_2^2 + x_3^2) = 14$$\n",
        "\n",
        "Gradient of $y$ with respect to $x$ (this is a vector)\n",
        "\n",
        "$$\\nabla y = \\left[\\frac{\\partial y}{\\partial x_1}\\space \\frac{\\partial y}{\\partial x_2} \\space \\frac{\\partial y}{\\partial x_3} \\right] = [2 x_1 \\space 2 x_2 \\space 2 x_3] = [2 \\space 4 \\space 6]$$\n",
        "\n",
        "Note: `.backward()` can only be called on a scalar (such as output of loss function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBPEPJftg8SJ",
        "outputId": "c26f750a-eed4-46ab-a141-d73867f04bf8"
      },
      "source": [
        "x = torch.tensor([1,2,3], dtype=torch.float32, requires_grad=True)\n",
        "y = (x**2).sum()\n",
        "y"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(14., grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n3JB1NVhjTu",
        "outputId": "0efdea25-f320-4596-83d8-8d176ae36f62"
      },
      "source": [
        "y.backward()\n",
        "x.grad"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu3saCrth8JE"
      },
      "source": [
        "Linear regression\n",
        "\n",
        "Input $\\vec{x}$\n",
        "\n",
        "$$\\vec{x} = [1 \\space 2 \\space 4]$$\n",
        "\n",
        "Linear projection (matrix multiplication, $W$ is a matrix)\n",
        "\n",
        "$$\\vec{y} = W\\vec{x}$$\n",
        "\n",
        "Loss\n",
        "\n",
        "$$L = MSE(\\vec{y}, \\vec{y_{true}}) = \\frac{(y_1 - y_{true,1})^2 + \\dots + (y_n - y_{true,n})^2}{n}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydEFOyjOi2i6",
        "outputId": "bc0f019c-57ba-4bc3-fb2f-c36befe46aeb"
      },
      "source": [
        "x = torch.tensor([1,2,4], dtype=torch.float32)          # don't set requires_grad=True here because this is input data  \n",
        "W = torch.randn((4,3), requires_grad=True)              # weight, sampled from normal distribution N(0,1)\n",
        "W"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6930,  0.7489,  0.3976],\n",
              "        [ 0.5207,  1.3825,  0.0206],\n",
              "        [-0.5965,  1.6456, -0.9929],\n",
              "        [-0.2841, -1.7786,  0.0020]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h3iU36WjD-n",
        "outputId": "e6292201-aced-4665-d37c-104f0e9b521a"
      },
      "source": [
        "y = torch.matmul(W, x)\n",
        "y"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2.3953,  3.3681, -1.2770, -3.8335], grad_fn=<MvBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44mmv-jzjTFd",
        "outputId": "e1ea7399-d76b-4e36-d9b4-ea2dfd61d29d"
      },
      "source": [
        "y_true = torch.tensor([1,2,3,4])\n",
        "y_true"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfbllenEjeXv",
        "outputId": "008133b3-52d9-4dfd-b729-d8bf84594466"
      },
      "source": [
        "loss = ((y - y_true)**2).mean()\n",
        "loss"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(20.8689, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iha5DTZPjlpc"
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oSQLSMkjpIN",
        "outputId": "c7155251-20bf-436d-b470-1afbfae59f52"
      },
      "source": [
        "W.grad"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.6976,   1.3953,   2.7905],\n",
              "        [  0.6840,   1.3681,   2.7362],\n",
              "        [ -2.1385,  -4.2770,  -8.5540],\n",
              "        [ -3.9168,  -7.8335, -15.6671]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUQDJoSSkTFx"
      },
      "source": [
        "Gradient descent ($\\alpha$ is the learning rate)\n",
        "\n",
        "$$W := W - \\nabla L \\cdot \\alpha$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soqPnM95jsbN",
        "outputId": "e158825e-394d-41ca-ad99-d960b92510f2"
      },
      "source": [
        "W = W.detach() - W.grad * 0.03\n",
        "W"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7139,  0.7070,  0.3139],\n",
              "        [ 0.5002,  1.3415, -0.0615],\n",
              "        [-0.5324,  1.7739, -0.7363],\n",
              "        [-0.1666, -1.5436,  0.4720]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcQGGt54kcTk"
      },
      "source": [
        "Loss is now smaller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR2PV2jBkPPQ",
        "outputId": "94e7b88a-2908-4ba6-a0d6-52016c56cd6f"
      },
      "source": [
        "y = torch.matmul(W, x)\n",
        "loss = ((y - y_true)**2).mean()\n",
        "loss"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9.7922)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnqyX0zhmdtn"
      },
      "source": [
        "2-layer Neural network\n",
        "\n",
        "Input $\\vec{x}$\n",
        "\n",
        "$$\\vec{x} = [1 \\space 2 \\space 4]$$\n",
        "\n",
        "Layer 1:\n",
        "\n",
        "- Linear projection (matrix multiplication, $W_1$ is a matrix)\n",
        "\n",
        "$$\\vec{y_1} = W_1\\vec{x}$$\n",
        "\n",
        "- Non-linear activation function (ReLU)\n",
        "\n",
        "$$\\vec{a_1} = ReLU(\\vec{y_1}) = ReLU(W_1\\vec{x})$$ \n",
        "\n",
        "Layer 2: (output layer, don't apply activation)\n",
        "\n",
        "- Linear projection\n",
        "\n",
        "$$\\vec{y} = \\vec{y_2} = W_2\\vec{a_1}$$\n",
        "\n",
        "Loss\n",
        "\n",
        "$$L = MSE(\\vec{y}, \\vec{y_{true}}) = \\frac{(y_1 - y_{true,1})^2 + \\dots + (y_n - y_{true,n})^2}{n}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyQ7IlnNjwsY",
        "outputId": "f7e0c863-40de-4a08-b163-aea48fd32e1d"
      },
      "source": [
        "x = torch.tensor([1,2,4], dtype=torch.float32)\n",
        "\n",
        "# layer 1\n",
        "W1 = torch.randn((4,3), requires_grad=True)     # initialize weight\n",
        "y1 = torch.matmul(W1, x)                        # linear projection\n",
        "a1 = torch.relu(y1)                             # apply relu\n",
        "\n",
        "print(\"y1:\", y1)\n",
        "print(\"a1:\", a1)\n",
        "\n",
        "# layer 2\n",
        "W2 = torch.rand((4,4), requires_grad=True)      # initialize weight\n",
        "y2 = torch.matmul(W2, a1)                       # linear projection\n",
        "\n",
        "print(\"y2:\", y2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y1: tensor([ 1.4714,  2.9440,  8.2453, -1.2481], grad_fn=<MvBackward>)\n",
            "a1: tensor([1.4714, 2.9440, 8.2453, 0.0000], grad_fn=<ReluBackward0>)\n",
            "y2: tensor([ 5.7500,  6.1875, 11.3977,  6.4486], grad_fn=<MvBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI_eGKrDkPD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45cf28c2-091d-490d-f8b0-feb5baf5b93c"
      },
      "source": [
        "loss = ((y2 - y_true)**2).mean()\n",
        "loss.backward()\n",
        "loss"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(29.1538, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdesqzrQoetx"
      },
      "source": [
        "Gradient of Loss with respect to layer 1's weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxDLL9vzkVv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "155802f9-f28d-4af4-f9b3-bc0d081af133"
      },
      "source": [
        "W1.grad"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 7.9427, 15.8855, 31.7709],\n",
              "        [ 6.1575, 12.3151, 24.6302],\n",
              "        [ 6.3732, 12.7465, 25.4929],\n",
              "        [ 0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k_K5IdAoipS"
      },
      "source": [
        "Gradient of Loss with respect to layer 2's weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4EBdvYBkW2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6554f4-1768-4887-c9bb-1e94366e833c"
      },
      "source": [
        "W2.grad"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.4945,  6.9921, 19.5826,  0.0000],\n",
              "        [ 3.0807,  6.1641, 17.2638,  0.0000],\n",
              "        [ 6.1781, 12.3615, 34.6208,  0.0000],\n",
              "        [ 1.8014,  3.6043, 10.0947,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScD4eqrLonM1"
      },
      "source": [
        "Gradient descent and see new loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wlESuS9nZTs",
        "outputId": "54357b1d-8433-4b45-bb1f-6fd0c2c57c06"
      },
      "source": [
        "W1 = W1 - W1.grad * 0.03\n",
        "W2 = W2 - W2.grad * 0.03\n",
        "\n",
        "y1 = torch.matmul(W1, x)\n",
        "a1 = torch.relu(y1)\n",
        "W2 = torch.rand((4,4), requires_grad=True)\n",
        "y2 = torch.matmul(W2, a1)\n",
        "\n",
        "loss = ((y2 - y_true)**2).mean()\n",
        "loss"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.7818, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZDzqEy2pCDQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}